#  Multi-Armed Bandit Algorithm

##  Descripci贸n
Es una implementaci贸n del algoritmo de bandidos multi-brazo (Multi-Armed Bandit) utilizando la estrategia **Epsilon-Greedy**. Es una t茅cnica fundamental en el aprendizaje por refuerzo, ideal para situaciones donde un agente debe equilibrar la exploraci贸n de nuevas opciones y la explotaci贸n de las conocidas.

##  Caracter铆sticas
- **Exploraci贸n y Explotaci贸n**: Usa Epsilon-Greedy para balancear entre probar nuevos bandidos y seleccionar el mejor conocido. Ideal para entender los conceptos b谩sicos del aprendizaje por refuerzo.

##  C贸mo Funciona
1. **Inicializaci贸n**: Se crean tres bandidos con diferentes probabilidades de dar una recompensa.
2. **Ejecuci贸n de Episodios**: Durante un n煤mero predefinido de episodios, se selecciona un bandido basado en la estrategia Epsilon-Greedy.
3. **Actualizaci贸n de Valores**: Despu茅s de cada episodio, se actualiza el valor esperado de recompensa del bandido seleccionado.

   \[ Q_{k+1}(a) = Q_{k}(a) + \frac{1}{k + 1}(r_{k+1} - Q_{k}(a)) \]

    Donde:
   - `Q_{k+1}(a)` es el nuevo valor esperado de recompensa para la acci贸n `a`.
   - `Q_{k}(a)` es el valor anterior.
   - `k` es el n煤mero de veces que la acci贸n `a` ha sido seleccionada hasta el momento.
   - `r_{k+1}` es la recompensa obtenida en la 煤ltima selecci贸n de la acci贸n `a`.

